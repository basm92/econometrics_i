---
title: "Homework Stat 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1:

Here, you will simulate some data to explore some of the properties of the OLS
estimator. Please submit answers as a RStudio Notebook containing code. Compute
all estimators using lm. At the beginning of the code, set the random seed to 810
using set.seed(). Failure to do so will be penalised.


```{}
library("stargazer")
library(purrr)
library(readr)
set.seed(810)
```

a) Simulate 100,000 observations

```{r pressure, echo=FALSE}
alpha = 5
x <- rnorm(100000, 6, sqrt(3))
e <- rnorm(100000, 0, sqrt(1))

y <- alpha + 0.3*x + 0.1 * x^2 + e

hist(y, breaks = 50, main = "Histogram of Yi")
hist(x, breaks = 50, main = "Histogram of Xi")
hist(x^2, breaks = 50, main = "Histogram of Xi^2")

plot(x, y, main = "Scatterplot of y against X", xlab = "X", ylab = "y")
```

b) Adding the squared x changes the mean and range of the distribution of the beta1 estimator. However, the mean and distribution of the standard errors is similar. 

```{r}
data <- data.frame(yValue = y,               
                   xValue = x,
                   epsilon = e)

data$xSQ <- '^'(data$xValue,2)

# For sequential grouping
groups<-1000
dataGroups <- split(data, factor(sort(rank(row.names(data))%%groups)))

regression1 <- list()
regression2 <- list()
standarderror1 <- list()
standarderror2 <- list()

for(i in 1:length(dataGroups)) {        # Loop from 1 to length of list
  subdata <- dataGroups[[i]]
  linear.1 <- lm(yValue ~ xValue, data=subdata)
  linear.2 <- lm(yValue ~ xValue + xSQ, data=subdata)
  beta1 <- unlist(linear.1[1])
  regression1[i] <- unname(beta1[2])
  beta2 <- unlist(linear.2[1])
  regression2[i] <- unname(beta2[2])
  standarderror1[i] <- (t(resid(linear.1)%*%resid(linear.1)))/(100-2)
  standarderror2[i] <- (t(resid(linear.2)%*%resid(linear.2)))/(100-3)
}

regression1 <- unlist(regression1)
hist(regression1, breaks = 50, main = "Histogram beta1")

regression2 <- unlist(regression2)
hist(regression2, breaks = 50, main = "Histogram of beta2")

standarderror1 <- unlist(standarderror1)
hist(standarderror1, breaks = 50, main = "Histogram of standard error of beta1")

standarderror2 <- unlist(standarderror2)
hist(standarderror2, breaks = 50, main = "Histogram of standard error of beta2")

```
c) We would violate assumption 1, as the sum of the square of the difference between Xi and X bar would be zero of Xi = c. Xi = c would also mean that X is not full rank and therefore not invertible. Hence, we could not calculate the least square estimators. 

d)

Comparing this histogram to thr histogram from b), we can see that the mean stays approximately the same. However, the spread and therefore the variance of the estimate from d) is smaller than from the estimate of b). This makes sense, as with an increasing number of observations the variance of the estimate decreases. The same can be observed with the standard error of the estimate. The mean stays approximately the same but the variance of the standard error decreases.

```{r}
alpha = 5
x <- rnorm(1000000, 6, sqrt(3))
e <- rnorm(1000000, 0, sqrt(1))

y <- alpha + 0.3*x + 0.1 * x^2 + e

data <- data.frame(yValue = y,               
                   xValue = x,
                   epsilon = e)

data$xSQ <- '^'(data$xValue,2)

# For sequential grouping
groups<-1000
dataGroups <- split(data, factor(sort(rank(row.names(data))%%groups)))

regression <- list()
standarderror <- list()

for(i in 1:length(dataGroups)) {        # Loop from 1 to length of list
  subdata <- dataGroups[[i]]
  linear <- lm(yValue ~ xValue + xSQ, data=subdata)
  beta <- unlist(linear[1])
  regression[i] <- unname(beta[2])
  standarderror[i] <- (t(resid(linear)%*%resid(linear)))/(1000-3)
}

regression <- unlist(regression)
hist(regression, breaks = 50, main = "Histogram beta")

standarderror <- unlist(standarderror)
hist(standarderror, breaks = 50, main = "Histogram standard error")

```
e) While the first regression has a different mean, the estimators of regression 2 and 3 have approximately the same mean and distribution. Therefore, the inclusion of c does not add any additional information to our estimation and hence is a redundant variable while including the square of x changes the estimation of beta 1, indicating that it is a non-linear relationship! We also can observe this with the standard errors of the estimate. The standard error of the second and third estimator has approximtely the same mean and distribution, indicating that c is a redundant variable. 

```{r}
x <- rnorm(100000, 6, sqrt(3))
e <- rnorm(100000, 0, sqrt(1))
c <- rnorm(100000, 1, sqrt(2)) + x
y <- alpha + 0.3*x + 0.1 * x^2 + e

data <- data.frame(yValue = y,               
                   xValue = x,
                   cValue = c,
                   epsilon = e)

data$xSQ <- '^'(data$xValue,2)

# For sequential grouping
groups<-1000
dataGroups <- split(data, factor(sort(rank(row.names(data))%%groups)))

regression1 <- list()
regression2 <- list()
regression3 <- list()
standarderror1 <- list()
standarderror2 <- list()
standarderror3 <- list()

for(i in 1:length(dataGroups)) {        # Loop from 1 to length of list
  subdata <- dataGroups[[i]]
  linear.1 <- lm(yValue ~ xValue, data=subdata)
  linear.2 <- lm(yValue ~ xValue + xSQ, data=subdata)
  linear.3 <- lm(yValue ~ xValue + xSQ + cValue, data=subdata)
  beta1 <- unlist(linear.1[1])
  regression1[i] <- unname(beta1[2])
  beta2 <- unlist(linear.2[1])
  regression2[i] <- unname(beta2[2])
  beta3 <- unlist(linear.3[1])
  regression3[i] <- unname(beta3[2])
  standarderror1[i] <- (t(resid(linear.1)%*%resid(linear.1)))/(100-2)
  standarderror2[i] <- (t(resid(linear.2)%*%resid(linear.2)))/(100-3)
  standarderror3[i] <- (t(resid(linear.3)%*%resid(linear.3)))/(100-4)
}

par(mfrow=c(3,1))

regression1 <- unlist(regression1)
hist(regression1, breaks = 50, main = "Histogram beta1")

regression2 <- unlist(regression2)
hist(regression2, breaks = 50, main = "Histogram of beta2")

regression3 <- unlist(regression3)
hist(regression3, breaks = 50, main = "Histogram of beta3")

par(mfrow=c(3,1))

standarderror1 <- unlist(standarderror1)
hist(standarderror1, breaks = 50, main = "Histogram of standard error of beta1")

standarderror2 <- unlist(standarderror2)
hist(standarderror2, breaks = 50, main = "Histogram of standard error of beta2")

standarderror3 <- unlist(standarderror3)
hist(standarderror3, breaks = 50, main = "Histogram of standard error of beta3")
```

# Question 4

a) From the first model, the variables age and smoker are significant at a p value of 0.1 The age has a positive effect on the birthweight, whereas the mother being a smoker has a negative effet. In this second model, the variable age is not significant anymore (and also negative), while smoker is still significant at a p value of 0.1. However, in comparison to the first model, its effect is smaller (although still very high). Of the added variables, the mother being unmarried is a significant variable and has a higher (negative) impact on the birthweight than the mother being a smoker. Education, on the other hand, has no significant effect. As we added two new independent variabels, it is only natural that R2 increases. Nonetheless, this does not mean that the new variables add to the explanation of the independet variable. 
```{r}
setwd("C:/Users/Anna/OneDrive/Dokumente/Tinbergen/TI_2021_B2/Econometrics 1")
my_data <- read_csv("DataAS1.csv")

model1 <- lm(birthweight ~ age + smoker + alcohol + drinks, data=my_data)
model2 <- lm(birthweight ~ age + smoker + alcohol + drinks + unmarried + educ, data=my_data)

stargazer(list(model1, model2),type="text",keep.stat = c("n","rsq"))
```
b) 

```{r}

RSS1 <- t(resid(model1))%*%resid(model1)
RSS1

RSS2 <- t(resid(model2))%*%resid(model2)
RSS2
# -------------------------------------------------------------------
# --- joint significance of minority and gender
# compute F-test: H0: b(unmarried)=b(educ)=0
# model 2 (model2): model under H0
g = 2 # number of restrictions
k = 6
n = nrow(my_data) # number of observations
Ftest = ((RSS1-RSS2)/g)/(RSS1/(n-k))
Ftest
1-pf(Ftest, g, n-k) #p-value for F(g,n-k)

# H0 is rejected at 5 % significance level
```

c) To test whether the residuals are normally distributed, we can use the Shapiro Test. The null hypothesis of the Shapiro test is that the residuals are normally distributed. Hence, the alternative hypothesis is that the residuals are not normally distributed. As the p value is below 0.05, we reject the null hypothesis. Therefore, there is evidence that the residuals are not normally distributed. 

```{r}
residuals2 <- unname(unlist(model2[2]))
hist(residuals2, breaks = 50, main = "Residuals Model 2")
shapiro.test(residuals2)
```

d) The t statistic of the non-linear model with squared fitted values gives us evidence that the model is non-linear as the null hypothesis is rejected. For the non-linear model with value^3, the F-statistics gives us evidence that the squared fitted values and values^3 are jointly non-zero (the t statistics on the other side, does not reject the null hypothesis for both estimators being zero independetly!).
```{r}
# non-linear models

n = nobs(model2)
C <- rep(1, n)
X <- cbind(C,my_data$age, my_data$smoker, my_data$alcohol, my_data$drinks, my_data$unmarried, my_data$educ)
bhat <- coefficients(model2)

predval <- X%*%bhat

# (1) non-linear model with squared fitted value
predval2 <- as.matrix(predval^2)

model3 <- lm(birthweight ~ age + smoker + alcohol + drinks + unmarried + educ + predval2, data=my_data)
summary(model3)

# non-linear term significant (t-test for H0: b(predval2)=0)
# reject H0

# (2) non-linear model with fitted value^3
predval3<-as.matrix(predval^3)

model4 <- lm(birthweight ~ age + smoker + alcohol + drinks + unmarried + educ + predval2 + predval3, data=my_data)
summary(model4)

RSS3 <- t(resid(model4))%*%resid(model4)

# compute F-test: H0: b(predval2)=b(predval3)=0
k = 9
g = 2 # number of restrictions
Ftest = ((RSS2-RSS3)/g)/(RSS3/(n-k))
Ftest
1-pf(Ftest, g, n-k) #p-value

# do not reject H0, there is evidence that predval2 and predval3 are jointly! not zero

```

e)

```{r}
model5 <- lm(log(birthweight) ~ age + smoker + alcohol + drinks + unmarried + educ, data=my_data)
```

