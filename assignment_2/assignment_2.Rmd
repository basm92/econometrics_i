---
title: "Assignment 2"
author: "630516am and 590049bm"
date: "23 Nov 2021"
output: pdf_document
---

```{r setup, include=FALSE}
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(echo = TRUE,
                      out.width = "250pt", 
                      out.height = "200pt", 
                      fig.align = "center",
                      size = 'small', 
                      warning = FALSE, 
                      message = FALSE)
f <- function(x) format(round(x, 2), big.mark=",")
gm <- list(
  list("raw" = "nobs", "clean" = "N"),
  list("raw" = "adj.r.squared", "clean" = "Adj. R2", fmt = f))
# ici on va mettre toutes les bibliotheques
library(tidyverse); library(modelsummary); library(car)
```

## Question 1

```{r}
set.seed(2021)
```


```{r}
b0 <- 3
b1 <- 5
b2 <- 8

gamma <- 1 

x1 <- rnorm(5000, mean = 1, sd = 1)
x2 <- rnorm(5000, mean = 2, sd = 1)

z <- rgamma(5000, shape = 1.2, scale = 1.1)

sigma_sq <- 1*exp(gamma*z)
epsilon <- rnorm(5000, mean = 0, sd = sqrt(sigma_sq))

y <- b0 + b1*x1 + b2*x2 + epsilon

```

```{r}
model1 <- lm(y ~ x1 + x2)

modelsummary(model1, 
             vcov = c("iid", "HC0"), 
             gof_map = gm, 
             stars = T)

```

```{r}
lmtest::bptest(formula = y ~ x1 + x2)
```

```{r}
gamma <- 0

sigma_sq <- 1*exp(gamma*z)
epsilon <- rnorm(5000, mean = 0, sd = sqrt(sigma_sq))

y <- b0 + b1*x1 + b2*x2 + epsilon
```


```{r}
model1 <- lm(y ~ x1 + x2)
model2 <- lm(y ~ x1 + x2, weights = sigma_sq)

```



## Question 2


1. Show that the OLS estimator of the parameter $\beta$ is not consistent. 

2. Derive plim (b) where b is the OLS estimator of $\beta$. Determine the sign of the magnitude of the inconsistency when $0 < \beta < 1$,  that is, the sign of plim(b) - $\beta$ when $0 < \beta < 1$.

First, we demean the two variables so that the constant-term $\alpha$ equals zero. Then we regress $\tilde{C} = \beta \tilde{D} + \epsilon$. We can do this because of Frisch-Waugh-Lovell. The estimate that we get is:


$$
\hat \beta = (\tilde{D}^T\tilde{D})^{-1}\tilde{D}^T C_ = (\tilde{D}^T \tilde{D})^{-1}(\beta  \tilde{D} + \epsilon) 
$$

and

$$
\mathbb{E}[\hat{\beta}] = \beta + (\tilde{D}^T \tilde{D})^{-1}\tilde{D}^T \epsilon 
$$
Evaluating the probability limit gives:

$$
\text{plim}_{n \rightarrow \infty} (\hat{\beta}) = \beta + \text{plim}(\frac{1}{n} \tilde{D}^T\tilde{D})^{-1} \cdot \text{plim}(\frac{1}{n}\tilde{D}^T\epsilon)
$$

which simplifies to:

$$
\beta + \frac{1}{\text{Var}(D)} \cdot \frac{1}{1-\beta} \sigma^2
$$
by the fact that variances and covariances are the same after demeaning, and by the reduced form equation for $D$ made explicit below. Under $0 < \beta < 1$, since variances are positives, the right term can only be positive and thus the bias is always positive. 

Substituting equation (2) into equation (1) and solving for $C$ gives:

$$
C = \frac{\alpha}{1-\beta} + \frac{\beta}{1-\beta} Z_i + \frac{1}{1-\beta} \epsilon_i
$$
substituting this back in the definition for $D$ gives:

$$
D = \frac{\alpha}{1-\beta} + \left(\frac{\beta}{1-\beta} + 1 \right) Z_i + \frac{1}{1-\beta} \epsilon_i
$$

From this, we can calculate Cov(D, $\epsilon_i$), which is $\frac{1}{1-\beta}\text{Var}(\epsilon)$. 
